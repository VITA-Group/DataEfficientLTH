{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Adversarial Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import models\n",
    "import os\n",
    "\n",
    "\n",
    "NETWORKS = {\n",
    "    \"resnet18\": models.resnet18,\n",
    "    \"resnet34\": models.resnet34,\n",
    "    \"resnet50\": models.resnet50,\n",
    "    \"resnet101\": models.resnet101,\n",
    "    \"resnet152\": models.resnet152,\n",
    "}\n",
    "\n",
    "DATA_SIZES = [\"1\", \"0.5\", \"0.2\", \"0.1\", \"0.02\", \"0.01\"]\n",
    "AUGS = [\"baseaug\", \"contrastaug\", \"randaug\", \"autoaug\"]\n",
    "ITERS = [0, 4, 9, 14]\n",
    "OUT_DIR = \"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = utils.add_args(parser)\n",
    "parser.add_argument(\"--eps\", type=float, default=8 / 255, help=\"fgsm attack eps\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "device, _ = utils.setup_device(False)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "metric_meter = utils.AvgMeter()\n",
    "\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        if not isinstance(mean, torch.Tensor):\n",
    "            mean = torch.tensor(mean)\n",
    "        if not isinstance(std, torch.Tensor):\n",
    "            std = torch.tensor(std)\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"std\", std)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        mean = self.mean[None, :, None, None]\n",
    "        std = self.std[None, :, None, None]\n",
    "        return inp.sub(mean).div(std)\n",
    "\n",
    "\n",
    "def eval(loader, model, metric_meter, attack=False):\n",
    "    metric_meter.reset()\n",
    "    model.eval()\n",
    "    for indx, (img, target) in enumerate(loader):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "\n",
    "        if attack:\n",
    "            img.requires_grad = True\n",
    "            pred = model(img)\n",
    "            cost = criterion(pred, target)\n",
    "            grad = torch.autograd.grad(cost, img, retain_graph=False, create_graph=False)[0]\n",
    "            adv_img = img + args.eps * grad.sign()\n",
    "            img = torch.clamp(adv_img, min=0, max=1).detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "        pred_cls = pred.argmax(dim=1)\n",
    "        acc = pred_cls.eq(target.view_as(pred_cls)).sum().item() / img.shape[0]\n",
    "\n",
    "        metrics = {\"loss\": loss.item(), \"acc\": acc}\n",
    "        metric_meter.add(metrics)\n",
    "        utils.pbar(indx / len(loader), msg=metric_meter.msg())\n",
    "    utils.pbar(1, msg=metric_meter.msg())\n",
    "\n",
    "\n",
    "f = open(f\"{args.dset}_rob_a_results.txt\", \"w\")\n",
    "for data_size in DATA_SIZES:\n",
    "    for aug in AUGS:\n",
    "        for iter in ITERS:\n",
    "            ckpt = os.path.join(\n",
    "                OUT_DIR, f\"sparse_advprop_{data_size}_{aug}\", f\"best_imp_{iter}.ckpt\"\n",
    "            )\n",
    "            print(f\"Evaluating: {ckpt}\")\n",
    "            ckpt = torch.load(ckpt)\n",
    "            if args.dset == \"cifar10\":\n",
    "                norm = Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                n_cls = 10\n",
    "            elif args.dset == \"cifar100\":\n",
    "                norm = Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "                n_cls = 100\n",
    "            else:\n",
    "                raise NotImplementedError(f\"args.dset = {args.dset} not implemented.\")\n",
    "            model = NETWORKS[args.net](n_cls=n_cls, pre_conv=\"small\", pretrained=False).to(device)\n",
    "            utils.modify_bn(model)\n",
    "            setattr(\n",
    "                model,\n",
    "                \"attacker\",\n",
    "                utils.PGDAttacker(args.attack_n_iter, args.attack_eps, args.attack_step_size, 0.2),\n",
    "            )\n",
    "            model = model.to(device)\n",
    "            if iter:\n",
    "                model.load_state_dict(ckpt[\"init\"])\n",
    "                curr_mask = utils.extract_mask(ckpt[\"model\"])\n",
    "                utils.mask_prune(model, curr_mask)\n",
    "                print(\"remaining weight = \", utils.check_sparsity(model))\n",
    "            model.load_state_dict(ckpt[\"model\"])\n",
    "            model = nn.Sequential(norm, model)\n",
    "            model = model.to(device)\n",
    "\n",
    "            # basic\n",
    "            transform = transforms.ToTensor()\n",
    "            dset = datasets.CIFAR10(\n",
    "                root=args.data_root,\n",
    "                train=False,\n",
    "                transform=transform,\n",
    "                download=True,\n",
    "            )\n",
    "            loader = DataLoader(\n",
    "                dset, batch_size=args.batch_size, shuffle=False, num_workers=args.n_workers\n",
    "            )\n",
    "            # eval(loader, model, metric_meter, attack=False)\n",
    "            # metrics = metric_meter.get()\n",
    "            # print(f\"{args.dset}: loss {round(metrics['loss'], 5)}, acc: {round(metrics['acc'], 5)}\")\n",
    "\n",
    "            eval(loader, model, metric_meter, attack=True)\n",
    "            metrics = metric_meter.get()\n",
    "            print(\n",
    "                f\"{args.dset}: loss {round(metrics['loss'], 5)}, acc: {round(metrics['acc'], 5)}\"\n",
    "            )\n",
    "\n",
    "            f.write(f\"{round(metrics['acc'], 4)*100}\" + \" \")\n",
    "            print(\"finished evaluating on ckpt\")\n",
    "            print(\"---------------------------\")\n",
    "        f.write(\"\\n\")\n",
    "        f.flush()\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Robutness to Distribution Shifts\n",
    "\n",
    "Download CIFAR10.2 from [https://github.com/modestyachts/cifar-10.2](https://github.com/modestyachts/cifar-10.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import models\n",
    "import os\n",
    "\n",
    "NETWORKS = {\n",
    "    \"resnet18\": models.resnet18,\n",
    "    \"resnet34\": models.resnet34,\n",
    "    \"resnet50\": models.resnet50,\n",
    "    \"resnet101\": models.resnet101,\n",
    "    \"resnet152\": models.resnet152,\n",
    "}\n",
    "\n",
    "DATA_SIZES = [\"1\", \"0.5\", \"0.2\", \"0.1\", \"0.02\", \"0.01\"]\n",
    "AUGS = [\"baseaug\", \"contrastaug\", \"randaug\", \"autoaug\"]\n",
    "ITERS = [0, 4, 9, 14]\n",
    "OUT_DIR = \"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = utils.add_args(parser)\n",
    "parser.add_argument(\n",
    "    \"--rob_data_root\", type=str, required=True, help=\"path to transformed data directory\"\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "device, _ = utils.setup_device(False)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "metric_meter = utils.AvgMeter()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(loader, model, metric_meter):\n",
    "    metric_meter.reset()\n",
    "    model.eval()\n",
    "    for indx, (img, target) in enumerate(loader):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "\n",
    "        pred = model(img)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        pred_cls = pred.argmax(dim=1)\n",
    "        acc = pred_cls.eq(target.view_as(pred_cls)).sum().item() / img.shape[0]\n",
    "\n",
    "        metrics = {\"loss\": loss.item(), \"acc\": acc}\n",
    "        metric_meter.add(metrics)\n",
    "        utils.pbar(indx / len(loader), msg=metric_meter.msg())\n",
    "    utils.pbar(1, msg=metric_meter.msg())\n",
    "\n",
    "\n",
    "class CIFARRobustness(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        train_imgs, train_labels = (\n",
    "            np.load(os.path.join(root, \"train.npz\"))[\"images\"],\n",
    "            np.load(os.path.join(root, \"train.npz\"))[\"labels\"],\n",
    "        )\n",
    "        test_imgs, test_labels = (\n",
    "            np.load(os.path.join(root, \"test.npz\"))[\"images\"],\n",
    "            np.load(os.path.join(root, \"test.npz\"))[\"labels\"],\n",
    "        )\n",
    "        self.imgs = np.concatenate([train_imgs, test_imgs], axis=0)\n",
    "        self.labels = np.concatenate([train_labels, test_labels], axis=0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        img = self.imgs[indx]\n",
    "        label = self.labels[indx]\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "f = open(f\"{args.dset}_rob_d_results.txt\", \"w\")\n",
    "for data_size in DATA_SIZES:\n",
    "    for aug in AUGS:\n",
    "        temp = []\n",
    "        for iter in ITERS:\n",
    "            ckpt = os.path.join(OUT_DIR, f\"sparse_{data_size}_{aug}\", f\"best_imp_{iter}.ckpt\")\n",
    "            print(f\"Evaluating: {ckpt}\")\n",
    "            ckpt = torch.load(ckpt)\n",
    "            if args.dset == \"cifar10\":\n",
    "                norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                n_cls = 10\n",
    "            elif args.dset == \"cifar100\":\n",
    "                norm = transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "                n_cls = 100\n",
    "            else:\n",
    "                raise NotImplementedError(f\"args.dset = {args.dset} not implemented.\")\n",
    "            model = NETWORKS[args.net](n_cls=n_cls, pre_conv=\"small\").to(device)\n",
    "            if iter:\n",
    "                model.load_state_dict(ckpt[\"init\"])\n",
    "                curr_mask = utils.extract_mask(ckpt[\"model\"])\n",
    "                utils.mask_prune(model, curr_mask)\n",
    "                print(\"remaining weight = \", utils.check_sparsity(model))\n",
    "            model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "            # basic\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    norm,\n",
    "                ]\n",
    "            )\n",
    "            dset = datasets.CIFAR10(\n",
    "                root=args.data_root,\n",
    "                train=False,\n",
    "                transform=transform,\n",
    "                download=True,\n",
    "            )\n",
    "            loader = DataLoader(\n",
    "                dset, batch_size=args.batch_size, shuffle=False, num_workers=args.n_workers\n",
    "            )\n",
    "            eval(loader, model, metric_meter)\n",
    "            metrics = metric_meter.get()\n",
    "            print(\n",
    "                f\"{args.dset}: loss {round(metrics['loss'], 5)}, acc: {round(metrics['acc'], 5)}\"\n",
    "            )\n",
    "\n",
    "            dset = CIFARRobustness(root=args.rob_data_root, transform=transform)\n",
    "            loader = DataLoader(\n",
    "                dset, batch_size=args.batch_size, shuffle=False, num_workers=args.n_workers\n",
    "            )\n",
    "            eval(loader, model, metric_meter)\n",
    "            metrics = metric_meter.get()\n",
    "            print(\n",
    "                f\"{args.dset}: loss {round(metrics['loss'], 5)}, acc: {round(metrics['acc'], 5)}\"\n",
    "            )\n",
    "            temp.append(str(round(metrics[\"acc\"], 4) * 100))\n",
    "\n",
    "            print(\"finished evaluating on ckpt\")\n",
    "            print(\"---------------------------\")\n",
    "\n",
    "        f.write(\" \".join(temp) + \"\\n\")\n",
    "        f.flush()\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Robustness to Synthetic Corruptions \n",
    "\n",
    "Download CIFAR10-C from [https://zenodo.org/record/2535967](https://zenodo.org/record/2535967)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import models\n",
    "import os\n",
    "\n",
    "NETWORKS = {\n",
    "    \"resnet18\": models.resnet18,\n",
    "    \"resnet34\": models.resnet34,\n",
    "    \"resnet50\": models.resnet50,\n",
    "    \"resnet101\": models.resnet101,\n",
    "    \"resnet152\": models.resnet152,\n",
    "}\n",
    "\n",
    "DATA_SIZES = [\"1\", \"0.5\", \"0.2\", \"0.1\", \"0.02\", \"0.01\"]\n",
    "AUGS = [\"baseaug\", \"contrastaug\", \"randaug\", \"autoaug\"]\n",
    "ITERS = [0, 4, 9, 14]\n",
    "OUT_DIR = \"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = utils.add_args(parser)\n",
    "parser.add_argument(\n",
    "    \"--rob_data_root\", type=str, required=True, help=\"path to transformed data directory\"\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "device, _ = utils.setup_device(False)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "metric_meter = utils.AvgMeter()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(loader, model, metric_meter):\n",
    "    metric_meter.reset()\n",
    "    model.eval()\n",
    "    for indx, (img, target) in enumerate(loader):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "\n",
    "        pred = model(img)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        pred_cls = pred.argmax(dim=1)\n",
    "        acc = pred_cls.eq(target.view_as(pred_cls)).sum().item() / img.shape[0]\n",
    "\n",
    "        metrics = {\"loss\": loss.item(), \"acc\": acc}\n",
    "        metric_meter.add(metrics)\n",
    "        utils.pbar(indx / len(loader), msg=metric_meter.msg())\n",
    "    utils.pbar(1, msg=metric_meter.msg())\n",
    "\n",
    "\n",
    "class CIFARRobustness(Dataset):\n",
    "    TYPES = [\n",
    "        # noise\n",
    "        \"gaussian_noise\",\n",
    "        \"shot_noise\",\n",
    "        \"impulse_noise\",\n",
    "        # blur\n",
    "        \"defocus_blur\",\n",
    "        \"glass_blur\",\n",
    "        \"motion_blur\",\n",
    "        \"zoom_blur\",\n",
    "        # weather\n",
    "        \"snow\",\n",
    "        \"frost\",\n",
    "        \"fog\",\n",
    "        \"brightness\",\n",
    "        # digital\n",
    "        \"contrast\",\n",
    "        \"elastic_transform\",\n",
    "        \"pixelate\",\n",
    "        \"jpeg_compression\",\n",
    "        # extra\n",
    "        \"gaussian_blur\",\n",
    "        \"saturate\",\n",
    "        \"spatter\",\n",
    "        \"speckle_noise\",\n",
    "    ]\n",
    "    LEVELS = [1, 2, 3, 4, 5]\n",
    "\n",
    "    def __init__(self, root, type, level, transform):\n",
    "        assert type in self.TYPES\n",
    "        assert level in self.LEVELS\n",
    "        imgs = np.load(os.path.join(root, f\"{type}.npy\"))\n",
    "        labels = np.load(os.path.join(root, \"labels.npy\"))\n",
    "        self.imgs = imgs[(level - 1) * 10_000 : level * 10_000]\n",
    "        self.labels = labels[(level - 1) * 10_000 : level * 10_000]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        img = self.imgs[indx]\n",
    "        label = self.labels[indx]\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "f = open(f\"{args.dset}_rob_s_results.txt\", \"w\")\n",
    "for data_size in DATA_SIZES:\n",
    "    for aug in AUGS:\n",
    "        for iter in ITERS:\n",
    "            ckpt = os.path.join(OUT_DIR, f\"sparse_{data_size}_{aug}\", f\"best_imp_{iter}.ckpt\")\n",
    "            print(f\"Evaluating: {ckpt}\")\n",
    "            ckpt = torch.load(ckpt)\n",
    "            if args.dset == \"cifar10\":\n",
    "                norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                n_cls = 10\n",
    "            elif args.dset == \"cifar100\":\n",
    "                norm = transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "                n_cls = 100\n",
    "            else:\n",
    "                raise NotImplementedError(f\"args.dset = {args.dset} not implemented.\")\n",
    "            model = NETWORKS[args.net](n_cls=n_cls, pre_conv=\"small\").to(device)\n",
    "            if iter:\n",
    "                model.load_state_dict(ckpt[\"init\"])\n",
    "                curr_mask = utils.extract_mask(ckpt[\"model\"])\n",
    "                utils.mask_prune(model, curr_mask)\n",
    "                print(\"remaining weight = \", utils.check_sparsity(model))\n",
    "            model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "            # basic\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    norm,\n",
    "                ]\n",
    "            )\n",
    "            dset = datasets.CIFAR10(\n",
    "                root=args.data_root,\n",
    "                train=False,\n",
    "                transform=transform,\n",
    "                download=True,\n",
    "            )\n",
    "            loader = DataLoader(\n",
    "                dset, batch_size=args.batch_size, shuffle=False, num_workers=args.n_workers\n",
    "            )\n",
    "            eval(loader, model, metric_meter)\n",
    "            metrics = metric_meter.get()\n",
    "            print(\n",
    "                f\"{args.dset}: loss {round(metrics['loss'], 5)}, acc: {round(metrics['acc'], 5)}\"\n",
    "            )\n",
    "\n",
    "            temp = []\n",
    "            for type in CIFARRobustness.TYPES:\n",
    "                for level in CIFARRobustness.LEVELS:\n",
    "                    dset = CIFARRobustness(\n",
    "                        root=args.rob_data_root, type=type, level=level, transform=transform\n",
    "                    )\n",
    "                    loader = DataLoader(\n",
    "                        dset, batch_size=args.batch_size, shuffle=False, num_workers=args.n_workers\n",
    "                    )\n",
    "                    eval(loader, model, metric_meter)\n",
    "                    metrics = metric_meter.get()\n",
    "                    print(\n",
    "                        f\"{args.dset} {type}_{level}: loss {round(metrics['loss'], 5)}, acc: {round(metrics['acc'], 5)}\"\n",
    "                    )\n",
    "                    temp.append(str(round(metrics[\"acc\"], 4) * 100))\n",
    "            f.write(\" \".join(temp) + \"\\n\")\n",
    "            f.flush()\n",
    "\n",
    "            print(\"finished evaluating on ckpt\")\n",
    "            print(\"---------------------------\")\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Layer-wise Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import models\n",
    "import os\n",
    "\n",
    "NETWORKS = {\n",
    "    \"resnet18\": models.resnet18,\n",
    "    \"resnet34\": models.resnet34,\n",
    "    \"resnet50\": models.resnet50,\n",
    "    \"resnet101\": models.resnet101,\n",
    "    \"resnet152\": models.resnet152,\n",
    "}\n",
    "\n",
    "DATA_SIZES = [\"1\", \"0.5\", \"0.2\", \"0.1\", \"0.02\", \"0.01\"]\n",
    "AUGS = [\"baseaug\", \"contrastaug\", \"randaug\", \"autoaug\"]\n",
    "ITERS = [4, 9, 14]\n",
    "OUT_DIR = \"\"\n",
    "# OUT_DIR = \"to_send/\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = utils.add_args(parser)\n",
    "args = parser.parse_args()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "CKPTS = [\n",
    "    # best winning tickets\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "for data_size in DATA_SIZES:\n",
    "    for iter in ITERS:\n",
    "        for aug in AUGS:\n",
    "            ckpt = os.path.join(OUT_DIR, f\"sparse_{data_size}_{aug}\", f\"best_imp_{iter}.ckpt\")\n",
    "            ckpt = torch.load(ckpt, map_location=torch.device(\"cpu\"))\n",
    "            model = NETWORKS[args.net](n_cls=10, pre_conv=\"small\")\n",
    "            if iter:\n",
    "                model.load_state_dict(ckpt[\"init\"])\n",
    "                curr_mask = utils.extract_mask(ckpt[\"model\"])\n",
    "                utils.mask_prune(model, curr_mask)\n",
    "            model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "            sparsity = []\n",
    "            for name, m in model.named_modules():\n",
    "                if isinstance(m, torch.nn.Conv2d):\n",
    "                    actual = float(m.weight.nelement())\n",
    "                    sparse = float(torch.sum(m.weight == 0))\n",
    "                    sparsity.append(str(round((1 - sparse / actual) * 100, 2)))\n",
    "            print(\" \".join(sparsity))\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
